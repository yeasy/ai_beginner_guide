# 第七章：从多模态到具身智能

## 7.1 多模态学习

> [!NOTE]
> **本讲核心**：真正的智能不应该是个"瞎子"或"聋子"。
> **一句话口诀**：以前是读天书，现在是看电影。

### 7.1.1 什么是"多模态"？

其实，我们人类就是天生的 **多模态（Multimodal）** 生物。
*   你看书时，是在处理 **文本**（Text）。
*   你看到红灯停下时，是在处理 **视觉**（Visual）。
*   你听到警报声时，是在处理 **听觉**（Audio）。

以前的 AI 很惨，它是个"单模态"的残疾人。
*   GPT-3 只能看懂文字，你给它发张图，它一脸懵逼。
*   ImageNet 模型只能看懂图，你问它"这张图好笑在哪里"，它只会冷冷地回答"猫：99%"。

**多模态 AI**，就是把眼睛、耳朵、嘴巴都装到一个脑子上，让它像人一样感受这个花花世界。

### 7.1.2 对齐（Alignment）：让图和文"牵手"

怎么让 AI 知道"猫"这个字，和"毛茸茸的图片"是一回事？
这就要提到一个神级算法：**CLIP**（OpenAI 2021 年发布）。

它的训练方法简单粗暴：
1.  从网上抓 4 亿张图片和对应的说明文字。
2.  强迫 AI 做"连连看"。
3.  如果 AI 把"狗的图片"和"猫的文字"连在一起，就狠狠惩罚。
4.  如果连对了，就给糖吃。

经过 4 亿次训练，AI 终于顿悟了：原来这种长相的东西叫"狗"。
这样，**图像世界** 和 **文字世界** 就在 AI 的脑子里 **"对齐"** 了。

### 7.1.3 GPT-5：看图写作文

现在的 GPT-5，不仅仅是认识"这是猫"。
你给它一张复杂的财务报表截图，它能瞬间把里面的数据读出来，算好账，还顺便给你分析一下公司的经营状况。
你拍一张冰箱里的剩菜，它能识别出有什么食材，然后给你推荐今晚的菜谱。

**它不仅仅在"看"，它在"理解"。**

### 思考题

现在的 AI 已经能看（视觉）、能听（听觉）、能读（文本）。
你觉得 AI 还需要哪种感官，才能真正理解人类世界？
（提示：为什么 AI 永远无法理解"妈妈做的红烧肉真好吃"？因为它缺了 **味觉** 和 **嗅觉**，也缺了 **情感**。）
