## 本章小结

本章全面介绍了 AI 技术的生态系统，帮助读者了解从硬件到应用的完整技术栈、主流平台和服务、开源工具，以及部署架构的选择。

### 核心要点回顾

**AI 技术栈的层次**
- 硬件层：GPU、TPU、专用 AI 芯片提供算力基础
- 基础设施层：CUDA、分布式训练、容器化支持高效运行
- 框架层：PyTorch、TensorFlow、JAX 是主流选择
- 模型层：基础模型和模型服务提供核心能力
- 应用层：面向用户的产品和服务

**主流 AI 平台与服务**
- 云厂商（AWS、Azure、GCP）提供全面的 AI 服务
- OpenAI、Anthropic、Google 是大语言模型领域的主要玩家
- 国内有文心一言、通义千问、Kimi 等选择
- 专业领域有图像、音频、代码等垂直服务

**开源框架与工具**
- PyTorch 是研究首选，TensorFlow 擅长生产部署
- Hugging Face 是模型和数据集的“GitHub”
- 开源大模型（LLaMA、Qwen 等）提供本地部署选择
- LangChain、向量数据库等构成 AI 应用开发工具链

**云端 AI 与边缘 AI**
- 云端：算力强、模型大，但依赖网络、有延迟和隐私顾虑
- 边缘：低延迟、保护隐私，但受设备限制
- 混合架构结合两者优势
- 端侧 AI 能力持续增强是主要趋势

### 关键术语

| 术语 | 解释 |
|------|------|
| GPU | 图形处理器，深度学习的主要计算硬件 |
| TPU | Google 的张量处理单元，专用 AI 芯片 |
| CUDA | NVIDIA 的并行计算平台 |
| PyTorch | Meta 开发的深度学习框架 |
| TensorFlow | Google 开发的深度学习框架 |
| Hugging Face | AI 模型和数据集的共享平台 |
| 边缘 AI | 在终端设备本地运行的 AI |
| 模型压缩 | 减小模型大小以适配边缘部署的技术 |

### 工具速查表

| 用途 | 推荐工具 |
|------|----------|
| 深度学习框架 | PyTorch（研究）、TensorFlow（部署） |
| 预训练模型 | Hugging Face Transformers |
| 实验跟踪 | Weights & Biases、MLflow |
| 本地大模型 | Ollama、LM Studio |
| LLM 应用开发 | LangChain、LlamaIndex |
| 向量数据库 | Chroma、Pinecone |
| 快速演示 | Gradio、Streamlit |

### 延伸思考

1. 随着端侧 AI 能力增强，云端 AI 服务商的商业模式会如何变化？

2. 开源模型与闭源模型的竞争格局会如何演变？

3. 对于一个新的 AI 项目，应该如何选择技术栈和部署架构？

### 下章预告

到了这里，我们已经参观完了 AI 的“工具间”和“厨房”。
你可能不仅要问：这些厉害的工具背后，到底藏着什么原理？为什么显卡能算出智能？
接下来，让我们推开 **“核心技术”** 的大门，去探寻 AI 大脑深处的秘密。下一章：机器学习原理。
