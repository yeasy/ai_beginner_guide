## 13.1 AI 的偏见与伦理

我们总以为机器是绝对理性的，但实际上，AI 就像一面镜子，照出了人类社会的偏见。

### 13.1.1 算法也会“歧视”吗？

是的，而且很常见。

**原因**：AI 是通过学习互联网上的数据长大的。如果数据本身包含偏见，AI 就会学会这种偏见。

**案例**：
1. **招聘筛选**：亚马逊曾开发一个 AI 简历筛选系统，结果发现它歧视女性。因为历史上科技行业的男性简历更多，AI 误以为“男性”是优秀的标准。
2. **人脸识别**：早期的某些人脸识别算法，对白人男性的识别率很高，但对深肤色女性的识别率很低。

### 13.1.2 刻板印象的放大

如果你让 AI “画一个医生”，它大概率会画一个男性；让它“画一个护士”，大概率是女性。

这就是 **刻板印象**。虽然在统计学上可能是事实，但如果 AI 在做决策（比如贷款审批、医疗诊断）时带入这种偏见，就会造成社会不公。

### 13.1.3 “对齐”难题

即使我们想教 AI “要做个好人”，也很难定义什么是“好”。

- 电车难题：该救一个人还是救五个人？
- 文化差异：在美国被认为是自由的言论，在其他国家可能是冒犯。

让 AI 的价值观与人类价值观保持一致，这被称为 **“对齐问题”（Alignment Problem）**，是目前 AI 研究最核心的领域之一。

### 13.1.4 思考题

如果让你设计一个“招聘 AI”的公平性规则，你会加入哪 2 条硬性约束，来降低性别或地域偏见？
