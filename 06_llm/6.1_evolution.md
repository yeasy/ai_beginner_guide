# 第六章：大语言模型详解

## 6.1 从 RNN 到 Transformer

大语言模型的诞生是自然语言处理（NLP）领域长期技术演进的结果。理解这段历史，有助于把握 LLM 的技术本质。

### 6.1.1 语言模型的基本概念

**什么是语言模型**

语言模型是一种能够预测下一个词的概率分布的模型。给定一段文本，语言模型可以告诉你最可能接下来出现的词是什么。

数学上：P(wₙ | w₁, w₂, ..., wₙ₋₁)

例如，给定"今天天气真"，语言模型可能预测：
- "好" - 60% 概率
- "差" - 20% 概率
- "热" - 10% 概率
- ...

**语言模型的应用**
- 文本生成（续写文章、对话）
- 拼写纠错
- 语音识别
- 机器翻译

### 6.1.2 RNN 与 LSTM 时代

**循环神经网络的尝试**

早期的神经语言模型使用 RNN 处理序列：
- 逐词处理文本
- 通过隐藏状态传递上下文信息
- 每个时刻预测下一个词

**LSTM 的改进**

LSTM 通过门控机制改善了长期依赖问题：
- 可以"记住"更长距离的信息
- 在机器翻译等任务上取得成功

**序列到序列（Seq2Seq）**

用于翻译等任务：
- 编码器将源语言编码为向量
- 解码器从向量生成目标语言

**RNN 的局限**
- 顺序处理，无法并行化
- 长序列仍然困难
- 计算效率低

### 6.1.3 注意力机制的诞生

2014-2015 年，注意力机制被引入 NLP：

**核心思想**

让模型在生成每个词时，"关注"输入中最相关的部分，而非依赖固定长度的向量表示。

**突破性进展**

在机器翻译中，翻译"时"可以直接关注源语言中相关的词，而非通过长距离传递。

### 6.1.4 Transformer 革命

2017 年，Google 发表论文"Attention Is All You Need"，提出 Transformer 架构。

**核心创新：自注意力**

完全抛弃 RNN，只使用注意力机制：
- 序列中每个位置可以直接关注任意其他位置
- 可以完全并行化处理
- 能够捕捉任意距离的依赖关系

**Transformer 的结构**

编码器-解码器结构，每个由多层组成：
- 多头自注意力层
- 前馈神经网络层
- 残差连接和层归一化

**位置编码**

由于 Transformer 本身不感知位置顺序，需要额外添加位置信息。

**Transformer 的优势**
- 训练速度快（可并行）
- 能处理长序列
- 扩展性好（可以增加层数和宽度）

### 6.1.5 BERT 与 GPT 的分歧

Transformer 诞生后，两条技术路线分化：

**BERT（2018，Google）**
- 双向编码器
- 擅长理解任务（分类、问答）
- 预训练任务：遮住词预测

**GPT（2018-，OpenAI）**
- 单向解码器
- 擅长生成任务
- 预训练任务：预测下一个词

**GPT 的胜出**

历史证明，GPT 的"预测下一个词"范式在扩展到足够大规模后，展现出惊人的通用能力，成为当今大语言模型的主流范式。
