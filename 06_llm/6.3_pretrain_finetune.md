## 6.3 预训练与微调

> 通用大模型是“通才”，垂直行业模型是“专才”。预训练练内功，微调练招式。

### 6.3.1 预训练（Pre-training）：通识教育

训练像 **大语言模型（LLM）** 这样的模型，第一步叫做 **预训练**。

这就是 **“上大学”**。

OpenAI 把全网的数据喂给它，不分学科，什么都学。物理、化学、编程、莎士比亚、八卦新闻...
这个阶段 **极其昂贵**（通常需要超大规模 GPU 集群，训练周期也可能按月计算）。

**目标**：让 AI 掌握人类的语言规律和世界的基本知识。
**结果**：它变成了一个博学多才，但不太懂规矩的“野人”。你问它“怎么制造炸弹”，它可能真的会告诉你。

### 6.3.2 后来微调（SFT）：岗前培训

这就需要第二步：**微调（Supervised Fine-Tuning）**。

这就是 **“岗前培训”**。

我们雇佣人类老师，写好标准的“问答对”（Instruction），教它如何对话，如何有礼貌，什么该说什么不该说。

这个阶段 **便宜** 得多。
**目标**：把“野人”变成“客服”、“医生”或“程序员”。
**结果**：它学会了遵循指令，变成了一个好用的助手。

### 6.3.3 RLHF：去人类社会实习

最后一步是 **人类反馈强化学习（RLHF）**。

这就是 **“实习”**。

如我们在 [4.4 节](../04_machine_learning/4.4_reinforcement.md)介绍的，RLHF 的常见做法是：先让人类标注员对多条候选回答做偏好排序，再用强化学习让模型更倾向于人类喜欢的回答。

这一步让 AI 逐步学会了什么该说、什么不该说，输出风格也更贴近人类的沟通习惯。

### 6.3.4 不只一条路：RAG、工具调用与工作流

在真实项目里，很多团队不会一上来就微调模型，而是先做：

1. **RAG（检索增强生成）**：先查企业私有资料，再让模型回答。  
2. **工具调用（Tool Use）**：让模型调用数据库、搜索、日历、ERP 等外部系统。  
3. **工作流编排（Workflow）**：把“检索 -> 生成 -> 校验 -> 审批”串成可追踪流程。

这三种方式通常比“从零训练/深度微调”更快落地、成本更低。

### 6.3.5 思考题

很多公司想拥有自己的“行业大模型”（比如法律大模型）。

他们通常不需要从头做“预训练”（太贵了），而是在通用模型的基础上做“微调”。

这就好比：你是想自己从幼儿园开始培养一个律师（预训练），还是直接招一个法学毕业生，给他培训两周你们公司的规定（微调）？

多数情况下会先选后者；但在不少场景里，RAG 和工具调用也能先解决 80% 的需求。
