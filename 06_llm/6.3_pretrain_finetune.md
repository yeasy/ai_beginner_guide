# 第六章：大语言模型详解

## 6.3 预训练与微调

大语言模型的训练是一个多阶段过程。理解这个过程，有助于把握 LLM 如何获得其强大能力。

### 6.3.1 预训练阶段

**目标**

在海量文本数据上进行自监督学习，学习语言的基本规律和世界知识。

**训练数据**

预训练使用的数据规模巨大：
- 网页内容（Common Crawl）
- 书籍
- 维基百科
- 代码仓库
- 学术论文

GPT-3 的训练数据约 45TB，包含数千亿词。

**训练任务**

预测下一个 token——这个看似简单的任务要求模型：
- 掌握语法和语义
- 理解世界知识
- 学会逻辑推理
- 习得多样化的能力

**计算成本**

预训练成本极高：
- 需要数千个 GPU 协同训练
- 训练时间可达数月
- 单次训练成本数百万美元

### 6.3.2 指令微调

**问题**

预训练后的模型很会"续写"，但不擅长遵循指令：
- 输入"写一首诗"可能续写为"的方法有..."
- 而非真正创作一首诗

**解决方案：指令微调**

在高质量的"指令-响应"对上进一步训练：
```
指令：用简单的语言解释量子力学
响应：量子力学是研究微观粒子行为的物理学分支...
```

**数据来源**
- 人工编写的高质量示例
- 众包收集的对话数据
- 从其他模型蒸馏

**效果**

指令微调后，模型能够：
- 理解并遵循用户指令
- 生成更有帮助的回复
- 适应不同任务类型

### 6.3.3 人类反馈强化学习（RLHF）

**动机**

即使经过指令微调，模型仍可能：
- 生成有害内容
- 回答不够有帮助
- 风格不够理想

需要进一步对齐人类偏好。

**RLHF 流程**

1. **收集人类反馈**
   - 模型生成多个回复
   - 人类标注者排序哪个更好

2. **训练奖励模型**
   - 学习预测人类的偏好
   - 输出"人类满意度"分数

3. **强化学习优化**
   - 使用奖励模型作为信号
   - 通过 PPO 等算法优化模型

**效果**

RLHF 显著提升了模型的：
- 安全性：减少有害输出
- 有帮助性：更好地满足用户需求
- 诚实性：减少编造和幻觉

ChatGPT 的成功很大程度上归功于 RLHF。

### 6.3.4 其他对齐技术

**Constitutional AI（CAI）**

Anthropic 提出的方法：
- 定义 AI 应遵循的"宪法"原则
- 让 AI 自我批评和修正
- 减少对人类标注的依赖

**DPO（Direct Preference Optimization）**

更简单的替代方案：
- 直接从偏好数据优化
- 无需训练单独的奖励模型
- 训练更稳定

### 6.3.5 参数高效微调

全参数微调成本高昂，发展出了高效替代方案：

**LoRA**

只训练少量额外参数：
- 冻结原始模型参数
- 添加低秩分解的适配层
- 参数量减少 90%+

**QLoRA**

结合量化和 LoRA：
- 模型以低精度加载
- 在量化基础上应用 LoRA
- 可在消费级 GPU 上微调

**Prompt Tuning**

只优化输入的可学习前缀：
- 模型完全冻结
- 只学习很少的参数
- 适合多任务场景

### 6.3.6 开源模型的微调生态

开源 LLM 催生了丰富的微调生态：

**基座模型**
- LLaMA 3
- Qwen 2
- Mistral

**微调工具**
- Hugging Face PEFT
- Axolotl
- LLaMA-Factory

**微调数据集**
- OpenOrca
- SlimOrca
- Alpaca

这使得即使是个人开发者也能定制自己的 LLM。
