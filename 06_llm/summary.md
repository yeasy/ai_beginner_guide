# 第六章：大语言模型详解

## 本章小结

本章深入解析了大语言模型的技术原理，从历史演进到核心机制，再到训练方法和主流模型。

### 核心要点回顾

**技术演进**
- 语言模型从统计方法发展到神经网络
- RNN/LSTM 解决了序列建模问题
- 注意力机制克服了长距离依赖困难
- Transformer 架构引发革命性突破

**工作原理**
- LLM 的核心任务是预测下一个 token
- 文本被切分为 token 后输入模型
- 自回归方式逐个生成文本
- 涌现能力随规模增长而出现

**训练范式**
- 预训练：在海量数据上学习语言和知识
- 指令微调：学会遵循指令
- RLHF：对齐人类偏好
- 参数高效微调：LoRA 等降低成本

**主流模型**
- 闭源：GPT-5、Claude 4、Gemini 2.0
- 开源：LLaMA 4、Qwen 2.5、Mistral
- 国内：文心一言、通义千问、DeepSeek

### 关键术语

| 术语 | 解释 |
|------|------|
| Token | 文本的基本处理单位 |
| 上下文窗口 | 模型一次处理的最大 token 数 |
| 自回归 | 逐个 token 生成的方式 |
| 预训练 | 在大规模数据上初始训练 |
| 指令微调 | 让模型学会遵循指令 |
| RLHF | 人类反馈强化学习 |
| LoRA | 低秩适配的高效微调方法 |

### 下章预告

下一章将介绍多模态与生成式 AI，探讨 AI 如何处理和生成图像、音频、视频等多种类型的内容，以及 DALL-E、Stable Diffusion、Sora 等生成式模型的技术原理。
