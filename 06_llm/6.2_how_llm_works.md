# 第六章：大语言模型详解

## 6.2 大语言模型的工作原理

大语言模型的能力令人惊叹，但其基本原理其实相当简单——预测下一个词。本节将解析 LLM 是如何工作的。

### 6.2.1 核心原理：下一个词预测

**极其简单的目标**

LLM 的核心任务只有一个：给定前面的文本，预测最可能的下一个词（token）。

```
输入：今天天气真
模型预测：好（概率最高）
```

**涌现的智能**

当这个简单任务在海量数据上训练到足够大规模时，模型"涌现"出了令人惊叹的能力：
- 语言理解和生成
- 逻辑推理
- 知识问答
- 代码编写
- ...

这种"量变引发质变"的现象被称为涌现能力（Emergent Abilities）。

### 6.2.2 Token 化

LLM 不直接处理文字，而是处理 token（词元）。

**什么是 Token**

Token 是文本被切分后的基本单位，可能是：
- 一个词："hello"
- 词的一部分："un" + "happy"
- 单个字符或中文字
- 标点符号

**常见分词方法**
- BPE（Byte Pair Encoding）
- WordPiece
- SentencePiece

**Token 数量**

中文每个字大约 1-2 个 token，英文每个词大约 1-4 个 token。GPT-4 的上下文窗口为 128K tokens，大约相当于 8-10 万中文字。

### 6.2.3 上下文窗口

**概念**

上下文窗口是模型一次能够处理的最大 token 数量。

**早期 vs 现代**
- GPT-3：2K tokens
- GPT-3.5：4K-16K tokens
- GPT-4：32K-128K tokens
- Claude 3：200K tokens

**长上下文的意义**
- 可以处理更长的文档
- 多轮对话保持更完整的记忆
- 可以"阅读"整本书再回答问题

### 6.2.4 生成过程

**自回归生成**

LLM 逐个 token 生成文本：
1. 输入提示词
2. 预测下一个最可能的 token
3. 将新 token 加入输入
4. 重复步骤 2-3 直到完成

```
输入：写一首关于春天的诗
→ 春
→ 春风
→ 春风吹
→ ...
```

**采样策略**

不总是选择概率最高的 token，而是引入随机性：

- **Temperature**：控制随机性程度
  - 低温度（0.2）：输出更确定、保守
  - 高温度（1.0+）：输出更随机、创意

- **Top-K**：只从概率最高的 K 个 token 中采样

- **Top-P（Nucleus）**：从累积概率达到 P 的 token 中采样

### 6.2.5 LLM 的能力与局限

**能力**
- 流畅的语言生成
- 广泛的知识覆盖
- 上下文学习（in-context learning）
- 指令遵循
- 一定程度的推理

**局限**
- **幻觉**：可能生成听起来正确但实际错误的内容
- **知识截止**：只知道训练数据中的信息
- **无法访问实时信息**（除非有工具）
- **推理能力有限**：复杂逻辑可能出错
- **无法学习新知识**：每次对话从零开始

### 6.2.6 涌现能力

**什么是涌现**

涌现是指在小模型中不存在，但当模型规模增大到一定程度时突然出现的能力。

**典型涌现能力**
- 多步推理
- 代码生成和理解
- 跨语言翻译
- 科学问题解答

**规模法则（Scaling Laws）**

研究发现，模型性能与三个因素呈可预测的幂律关系：
- 模型参数量
- 训练数据量
- 训练计算量

这意味着可以通过增加规模来持续提升模型能力——这正是"大"语言模型的由来。
