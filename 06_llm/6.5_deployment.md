# 6.5 大模型的部署与推理：让模型跑起来

当我们在网页上向大模型提问，几秒钟后它就开始流利地回答我们时，你是否好奇过它在云端到底是怎样运行的？

在真实的工业级环境中，让大模型“跑起来”的过程被称为**推理 (Inference)**。这并不是简单地在电脑上运行一段脚本程序，而是一个需要调用成千上万张昂贵显卡、极其复杂的系统工程。

## 6.5.1 算力与显存：谁更重要？

很多人以为大模型运行最耗费的是计算能力（算力），也就是 GPU 的处理速度。但在实际应用中，真正“卡脖子”的往往是**显存容量**和**显存带宽**。

大模型在回答我们的问题时，它的工作可以分为截然不同的两个阶段：

1. **阅读阶段（预填充 Prefill）**：模型会一目十行地阅读你发送的所有背景资料和提示词。这个阶段**极度消耗算力**，GPU 会开足马力进行并发的矩阵运算。
2. **回答阶段（解码 Decode）**：模型像一台打字机一样，一个字一个字地往外蹦答案。因为必须等上一个字生成后，才能去猜下一个字，这个阶段系统其实**无法充分利用算力**。这时候 GPU 大部分时间处于闲置状态，主要时间都花在了从显存里把庞大的模型权重来回搬运上。这就好比一个跑得飞快的厨师，却被狭窄的厨房运菜通道挡住了，只能慢吞吞地干活。

## 6.5.2 吞噬显存的怪兽：KV Cache

为了不让模型在每生成一个新词时，都必须把前面的整段话重新理解一遍，工程师们发明了“记事本”机制（叫做 **KV Cache**）。模型会把刚才读过和写过的字的中间状态，都存进这个显存记事本里。

但这带来了恐怖的显存消耗：你发送的文本越长（上下文越长），且同时请求的人数越多，这个记事本就越大。一台价值二三十万的顶级 80GB 显卡，光是把几千亿参数的模型装进去就塞得满满当当了，几乎没有额外的空间来存放几百个并发用户的记事本。

如果没有先进的显存管理技术（比如像现代电脑内存管理一样的显存分页技术 PagedAttention），大量的显存就会变成无法使用的碎片并被白白浪费，这甚至会导致系统直接崩溃（也就是我们常说的 OOM - 内存溢出）。

## 6.5.3 把显卡组装成超级大脑：分布式推理

既然单张甚至单台机器的显卡装不下，我们就必须用网络把十几台、甚至几百台服务器上的显卡连接起来一起算，这叫做**分布式部署**。

它跑起来就像一家大型数据工厂在进行极其默契的流水线作业：
- **切分模型**：这半边神经网络交由显卡A计算，那半边交由显卡B计算。它们彼此之间通过极其昂贵且高速的内部网络（比如 NVLink 或专用的高速以太网）在微秒级别交换数据。
- **解耦工厂**：一些最前沿的 AI 公司，甚至会将整个数据中心切分为两波。一波机器纯粹负责“阅读长文”（计算集群），另一波机器纯粹负责“逐字打字”（显存带宽集群）。这就像餐厅的前台负责极速点单和备菜，后厨负责快速出餐，以此达到极高的接待效率。

**总结**：在几百亿、几千亿参数的巨兽面前，隐藏在幕后的“推理技术”决定了 AI 公司的成本底线与回答速度。当你每次惊叹于大模型一秒钟能写出几十个词的回复时，要知道，这背后是成百上千张昂贵显卡在高速光缆的连接下，进行着精妙绝伦的硬件接力赛。

---

**下一节**: [本章小结](summary.md)
