## 5.3 深度学习架构

> 不同的器官负责不同的功能。AI 也是如此，不同的架构负责不同的数据。看图用卷积，听话用循环，万事不决转 Transformer。

### 5.3.1 CNN：计算机的“眼睛”

**卷积神经网络（CNN）** 是专门为 **图像** 而生的。

通过它，计算机终于“看”懂了世界。

它的核心法宝叫 **“卷积核”（Filter）**。

想象你拿一个小的方形放大镜，在图片上从左到右、从上到下地扫描。
*   第一层扫描：发现了“横线”、“竖线”（边缘）。
*   第二层扫描：发现线条组成了“圆圈”、“方块”（形状）。
*   第三层扫描：发现圆圈组成了“眼睛”、“轮子”（物体部件）。

CNN 的哲学是：**局部这一小块像素，比整张图更重要。**
正是因为 CNN，我们才有了人脸识别、自动驾驶、医学影像 AI。

### 5.3.2 RNN：计算机的“耳朵”

如果说图像是 **空间** 的（上下左右），那么语言就是 **时间** 的（先后顺序）。

“我爱你”和“你爱我”，字完全一样，但顺序变了，意思就反了。

处理这种序列数据，就要靠 **RNN（循环神经网络）**。

RNN 有一个 **“记忆条”**。

处理“爱”这个字时，它脑子里还留着“我”的残影；处理“你”时，它还记得前面的“我爱”。

正因为有了记忆，它才能由上文推断下文，才能做翻译、语音识别。

但 RNN 有个大毛病：**记性不好**。

句子太长（比如隔了 500 字），它就忘了开头讲啥了。这也是早期机器翻译总是驴唇不对马嘴的原因。

### 5.3.3 Transformer：全能的“大脑”

2017 年，Google 一篇论文《Attention Is All You Need》，改变了 AI 的历史。
**Transformer** 横空出世。

它抛弃了 RNN 那种“一个字一个字读”的慢节奏，直接 **“一目十行”**（并行计算）。

它用 **“注意力机制”（Attention）**，就像探照灯一样，能瞬间聚焦到整篇文章里最相关的那个词，不管它离得有多远。

Transformer 是如此强大，它不仅横扫了 NLP（语言），现在还杀进了 CV（视觉）。

它可以说是现在的 **大一统架构**。GPT、Claude、Sora，骨子里流的都是 Transformer 的基因。

### 5.3.4 思考题

人类的进化是先有眼睛耳朵（感官），最后才进化出强大的大脑皮层（思维）。

AI 的发展似乎也是这样：先搞定 CNN（视觉识别），再搞定 Transformer（理解思维）。

那么下一阶段的进化会是什么？是 **身体（机器人）** 吗？
