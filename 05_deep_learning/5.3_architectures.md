# 第五章：深度学习揭秘

## 5.3 主流深度学习架构

不同类型的数据和任务需要不同的网络架构。本节介绍最重要的几种架构。

### 5.3.1 卷积神经网络（CNN）

CNN 专门设计用于处理具有网格结构的数据，如图像。

**核心概念**

**卷积层**：使用卷积核（滤波器）在图像上滑动，提取局部特征：
- 共享参数，大幅减少参数量
- 保留空间结构信息
- 不同卷积核提取不同特征

**池化层**：降低特征图的空间维度：
- 减少计算量
- 带来一定的平移不变性
- 常用最大池化和平均池化

**典型 CNN 结构**
```
输入图像 → [卷积 → 激活 → 池化] × N → 全连接 → 输出
```

**经典 CNN 模型**

| 模型 | 年份 | 核心贡献/特点 |
|------|------|---------------|
| **LeNet** | 1998 | CNN 鼻祖，用于手写数字识别 |
| **AlexNet** | 2012 | 引入 ReLU 和 Dropout，开启深度学习革命 |
| **VGG** | 2014 | 证明网络越深效果越好，结构规整 |
| **ResNet** | 2015 | 引入残差连接，解决了深层网络退化问题 |
| **EfficientNet** | 2019 | 提出了复合缩放方法，平衡速度与精度 |

### 5.3.2 循环神经网络（RNN）

RNN 专门设计用于处理序列数据。

**核心思想**

RNN 有"记忆"能力——当前输出不仅取决于当前输入，还取决于之前的信息：
```
隐藏状态ₜ = f(隐藏状态ₜ₋₁, 输入ₜ)
```

**问题：梯度消失/爆炸**

原始 RNN 在处理长序列时，梯度会消失或爆炸，难以学习长期依赖。

**LSTM（长短期记忆）**

引入门控机制解决长期依赖问题：
- **遗忘门**：决定丢弃什么信息
- **输入门**：决定存储什么新信息
- **输出门**：决定输出什么信息

**GRU（门控循环单元）**

LSTM 的简化版本，参数更少，效果相当。

### 5.3.3 Transformer

Transformer 是当前最重要的架构，GPT、BERT、Claude 等都基于它。

**注意力机制**

Transformer 的核心是自注意力（Self-Attention）机制：
- 序列中每个位置都可以直接关注其他任意位置
- 不像 RNN 需要顺序处理
- 可以捕捉任意距离的依赖关系

**Query-Key-Value 框架**

注意力计算过程：
1. 将输入转换为 Query（查询）、Key（键）、Value（值）
2. 计算 Query 和所有 Key 的相似度
3. 用相似度对 Value 加权求和

**多头注意力**

使用多个注意力"头"，每个关注不同方面的信息。

**Transformer 结构**

- **编码器**：理解输入序列
- **解码器**：生成输出序列
**Transformer 家族分类**

| 架构类型 | 代表模型 | 典型应用 |
|----------|----------|----------|
| **纯编码器 (Encoder-only)** | BERT, RoBERTa | 文本分类、情感分析、阅读理解 |
| **纯解码器 (Decoder-only)** | GPT 系列, LLaMA | 文本生成、对话系统 |
| **完整结构 (Encoder-Decoder)** | T5, BART | 机器翻译、文本摘要 |

**Transformer 的优势**
- 并行计算能力强
- 有效捕捉长距离依赖
- 易于扩展到大规模
- 不仅用于 NLP，还扩展到视觉、音频等领域

### 5.3.4 其他重要架构

**生成对抗网络（GAN）**
- 由生成器和判别器组成
- 两者相互对抗，共同进步
- 用于图像生成

**变分自编码器（VAE）**
- 学习数据的概率分布
- 可以生成新样本
- 潜空间平滑连续

**扩散模型（Diffusion Models）**
- 学习去噪过程
- 当前图像生成的主流方法
- DALL-E、Stable Diffusion 的基础

### 5.3.5 架构选择指南

| 数据类型 | 推荐架构 |
|----------|----------|
| 图像 | CNN、ViT（Vision Transformer） |
| 序列 | Transformer（首选）、LSTM/GRU |
| 图结构 | 图神经网络（GNN） |
| 生成任务 | Transformer、扩散模型 |
| 表格数据 | 传统 ML 或简单神经网络 |
