## 本章小结

本章深入解析了深度学习的核心原理，从神经网络基础到关键技术，再到主流架构和实践挑战。

### 核心要点回顾

**神经网络基础**
- 人工神经元模拟生物神经元的工作方式
- 网络由输入层、隐藏层和输出层组成
- 激活函数引入非线性能力
- 前向传播处理数据，反向传播计算梯度
- “深度”使网络能学习层次化表示

**深度学习关键技术**
- 优化算法：SGD、Adam 等
- 正则化：Dropout、批归一化、权重衰减
- 初始化策略：Xavier、He 初始化
- 学习率调度：阶梯下降、余弦退火、预热
- 数据增强和迁移学习

**主流架构**
- CNN：处理图像，使用卷积提取特征
- RNN/LSTM：处理序列，具有记忆能力
- [Transformer](5.3_architectures.md)：基于注意力机制，当前最重要的架构

**训练挑战**
- 计算资源需求大
- 过拟合与泛化问题
- 梯度消失和爆炸
- 超参数调优困难
- 可解释性不足

### 关键术语

| 术语 | 解释 |
|------|------|
| 激活函数 | 引入非线性的函数 |
| 反向传播 | 计算梯度的算法 |
| 梯度下降 | 优化参数的方法 |
| 卷积 | 提取局部特征的操作 |
| 注意力机制 | Transformer 的核心 |
| 迁移学习 | 复用预训练模型的知识 |

### 下章预告

下一章将专门介绍大语言模型（LLM），深入解析 ChatGPT、Claude 等模型背后的技术原理，从 Transformer 架构到预训练与微调方法，帮助读者理解生成式 AI 的核心技术。
