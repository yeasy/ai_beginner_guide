## 本章小结

本章深入解析了深度学习的核心原理，从神经网络基础到关键技术，再到主流架构和实践挑战。

### 核心要点回顾

**神经网络基础**
- 人工神经元模拟生物神经元：接收数字 -> 加权求和 -> 激活函数 -> 传递结果
- “深度”就是网络的层数，每一层把信息进行一次抽象
- 激活函数（如 ReLU）引入非线性，让网络能模拟复杂事物
- 亿万个简单神经元连接起来，可以产生“涌现”现象

**深度学习训练**
- 梯度下降：在“黑夜里下山”，沿着下坡方向更新参数
- 学习率：步子太大会震荡，步子太小会太慢，最佳策略是“先快后慢”
- 局部最优陷阱：SGD 和 Adam 等方法通过加“惯性”来跳出小坑

**主流架构**
- CNN（卷积神经网络）：计算机的“眼睛”，专为图像而生
- RNN（循环神经网络）：计算机的“耳朵”，用于序列数据，但记性不好
- Transformer：全能的“大脑”，用注意力机制“一目十行”，是当前的大一统架构

**深度学习的局限性**
- 黑盒问题：无法解释内部推理过程，对关键领域是合规风险
- 能耗问题：训练顶级大模型需要极高电力和算力成本
- 幻觉问题：AI 会一本正经地编造假答案

### 关键术语

| 术语 | 解释 |
|------|------|
| 激活函数 | 引入非线性的函数（如 ReLU） |
| 梯度下降 | 沿“下坡”方向优化参数的方法 |
| 学习率 | 控制每一步参数更新幅度的超参数 |
| 卷积 | CNN 中提取局部特征的操作 |
| 注意力机制 | Transformer 的核心，能聚焦最相关的信息 |
| 幻觉 | AI 一本正经地编造虚假内容 |

### 下章预告

下一章将专门介绍大语言模型（LLM），深入解析 ChatGPT、Claude 等模型背后的技术原理，从 Transformer 架构到预训练与微调方法，帮助读者理解生成式 AI 的核心技术。
