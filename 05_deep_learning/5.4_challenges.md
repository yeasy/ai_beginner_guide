## 5.4 深度学习的局限性

> 深度学习虽然强大，但它有两个致命死穴：不可解释（黑箱）和不可持续（能耗）。知其然，不知其所以然。

### 5.4.1 黑箱：你为什么说是猫？

如果你的医生告诉你：“你得了这个病，但我不知道为什么，是我的 AI 助手说的，它看了一眼你的片子就确诊了。”

你敢信吗？

这就是深度学习最大的问题：**不可解释性（Black Box）**。

神经网络内部有几千亿个参数，就像一个巨大而混乱的线团。虽然它输出了正确答案，但人类无法理清它中间的逻辑路径。

在金融风控、自动驾驶、医疗诊断这些 **“人命关天”** 的领域，黑箱是一个巨大的合规风险。

### 5.4.2 吞金兽：不仅烧钱，还烧碳

训练顶级大模型常常需要极高电力和算力成本。

其碳排放也可能达到传统软件系统难以相比的量级。

深度学习是一种 **“暴力美学”**：大力出奇迹。虽然效果好，但它极其低效。

人脑只消耗 20W 功率，就能写出《红楼梦》；AI 消耗兆瓦级功率，可能还在算错一道数学题。

这种 **“能源换智能”** 的模式，长期来看是不可持续的。

### 5.4.3 数据饥渴：有多少人工，就有多少智能

深度学习模型是个不折不扣的“吃货”，不仅要吃算力，更要吃海量的高质量数据。

如果是没人标注过的稀缺领域数据，模型往往表现糟糕。“有多少人工，就有多少智能”这句玩笑话，在很多特定场景下依然成立。

### 5.4.4 灾难性遗忘：学了新知识，忘了老朋友

如果你让一个训练好的模型去学习新任务（比如学完了识别猫狗，再去学识别飞机），它很可能会把怎么识别猫狗给忘了。

这种“狗熊掰棒子”的现象被称为 **灾难性遗忘（Catastrophic Forgetting）**，说明它缺少人类那种举一反三和持续学习的机制。

### 5.4.5 幻觉：一本正经地胡说八道

当你问 AI 一个它不知道的问题，它不会说“我不知道”，而是会编造一个看起来非常有道理的假答案。

这叫 **“幻觉”（Hallucination）**。

因为 AI 本质上是在做“文字接龙”（详见后文 [6.2 节](../06_llm/6.2_how_llm_works.md)），它只管概率上通顺，不管事实上真假。

这是大模型刻在基因里的缺陷，目前还没有完美的解药。

### 5.4.6 思考题

我们是否真的需要 AI 具有“可解释性”？

我们自己的大脑其实也是个黑箱（你也不知道你是怎么突然想到一个好点子的，对吧？）。

如果 AI 能保证 99.999% 的准确率，远高于人类专家，我们是否可以接受它是一个黑箱？
