## 5.2 深度学习训练

> 训练模型就像在黑夜里下山，目标是找到山谷最低点（损失最小）。下山要看路，步子别太大。

### 5.2.1 梯度下降：AI 是怎么“悔过”的

我们之前说，训练就是“犯错挨打”。

但关键是，挨打之后，怎么改？

这就要用到 **梯度下降（Gradient Descent）**。

想象一下：你被直升机扔到了一座崇山峻岭中，四周漆黑一片（高维空间），你的目标是找到海拔最低的山谷（Loss = 0）。

你看不见路，只能用脚试探周围的坡度（计算梯度）。
*   如果你感觉那边是下坡，就往那边迈一步。
*   这就是 **“更新参数”**。

只要你不停地往“下坡”的方向走，理论上总能走到谷底。

### 5.2.2 学习率：步子迈多大？

在下山过程中，有一个生死攸关的参数：**学习率（Learning Rate）**。

它决定了你一步迈多远。

*   **步子太大**（学习率高）：虽然走得快，但容易直接跨过山谷，跑到对面山上去了（震荡不收敛）。
*   **步子太小**（学习率低）：像蚂蚁挪窝一样，走了一万年还没走到谷底（训练太慢）。

所以，最好的策略是 **“先快后慢”**：在山顶时步伐大一点，快到底时步伐小一点，慢慢挪进去（学习率衰减）。

### 5.2.3 局部最优：陷在半山腰

梯度下降有一个致命的缺陷：**局部最优陷阱**。

有时候你以为你到底了（四周都是上坡），但其实你只是在一个半山腰的小坑里。真正的谷底还在几公里外。

为了解决这个问题，科学家发明了各种“魔改版”下山法，比如 **SGD（随机梯度下降）** 和 **Adam**。

它们就像给你的下山过程加了一点 **“惯性”（动量）**。

当你冲进小坑时，利用惯性直接冲出去，继续寻找真正的大海。

> [!NOTE]
> **可观测的过拟合与早停策略**
> 
> 从工程角度看，只讲“梯度下降”是不够的。模型训练最大的敌人是**过拟合（Overfitting）**——模型在训练题上拿了满分，遇到没见过的新题却一塌糊涂。
> 1. **可观测证据**：我们需要同时绘制两条跟踪曲线：**训练集 Loss 曲线**（通常持续下降）和 **验证集（Validation Set）Loss 曲线**。一旦发现验证集 Loss 降到某一点后开始停滞甚至**反弹上升**，这就是模型“死记硬背”导致泛化能力崩溃的铁证。
> 2. **早停与回调（Early Stopping）**：在监控到上述的“剪刀差”现象，即验证集 Loss 连续 N 轮没有改善时，框架（如 PyTorch Lightning / Keras）通过代码强制触发拦截停止训练，并回滚恢复到保存的最佳权重状态（Checkpoint）。
> 
> **代码实验**：  
> 直观可执行的过拟合与早停机制观察代码，可参阅 [`labs/03_dl_overfitting.py`](../labs/03_dl_overfitting.py)。该脚本通过故意缩减数据量，为您稳定复现出训练与验证曲线产生劈叉的典型“过拟合”时刻。

### 5.2.4 思考题

为什么现在的 AI 工程师，经常自嘲是 **“炼丹师”**？

因为调节这些超参数（学习率、层数、Batch Size）就像配火药一样，往往没有绝对的科学公式，全靠经验和直觉（还有运气）。

你觉得未来会有“AI 炼丹师”这个职业吗？还是说 AI 最终能学会“自己调节参数”（AutoML）？
