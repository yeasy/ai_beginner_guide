# 第五章：深度学习揭秘

## 5.2 深度学习的关键技术

训练深度神经网络面临诸多挑战。本节介绍使深度学习成功的关键技术。

### 5.2.1 优化算法

**随机梯度下降（SGD）**

不使用全部数据计算梯度，而是每次随机抽取一小批数据：
- 降低计算成本
- 引入随机性，有助于跳出局部最优
- 收敛速度更快

**动量（Momentum）**

在梯度方向上累积"惯性"：
- 加速收敛
- 减少震荡
- 帮助穿越平坦区域

**Adam 优化器**

结合动量和自适应学习率，是目前最常用的优化器：
- 为每个参数计算自适应学习率
- 结合了一阶和二阶动量估计
- 参数设置较少依赖经验

### 5.2.2 正则化技术

防止过拟合的技术：

**Dropout**

训练时随机"丢弃"部分神经元：
- 每次训练只用部分网络
- 相当于训练多个子网络的集成
- 推理时使用完整网络

**权重衰减（L2 正则化）**

在损失函数中添加参数大小的惩罚项：
- 鼓励较小的参数值
- 使模型更简单
- 减少过拟合

**批归一化（Batch Normalization）**

对每层输入进行标准化：
- 加速训练收敛
- 允许使用更高学习率
- 有一定正则化效果

### 5.2.3 初始化策略

参数初始化对训练成功至关重要：

**Xavier 初始化**
- 适用于 Sigmoid 和 Tanh 激活函数
- 保持前向和反向传播时方差稳定

**He 初始化**
- 专为 ReLU 设计
- 考虑了 ReLU 的特性

**预训练初始化**
- 使用预训练模型的参数作为起点
- 迁移学习的基础

### 5.2.4 学习率调度

动态调整学习率：

**阶梯式下降**
- 每隔固定轮次降低学习率
- 简单直接

**余弦退火**
- 学习率按余弦曲线下降
- 平滑的衰减过程

**预热（Warmup）**
- 训练初期使用较小学习率
- 逐渐增加到正常值
- 避免初始不稳定

### 5.2.5 数据增强

通过对训练数据进行变换，人工增加数据量：

**图像增强**
- 随机裁剪、旋转、翻转
- 颜色抖动、亮度调整
- 混合图像（Mixup、CutMix）

**文本增强**
- 同义词替换
- 随机插入、删除、交换
- 回译（翻译到其他语言再翻译回来）

### 5.2.6 迁移学习

利用在大数据集上预训练的模型：

**工作原理**
1. 在大数据集（如 ImageNet）上预训练模型
2. 将预训练模型作为起点
3. 在目标任务上微调

**优势**
- 大幅减少所需训练数据
- 加速训练过程
- 提升最终效果

**应用场景**
- 数据量有限的任务
- 计算资源受限
- 快速原型开发
