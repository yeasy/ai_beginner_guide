## 5.2 深度学习训练

> **说明**
> **本讲核心**：训练模型就像在黑夜里下山，目标是找到山谷最低点（损失最小）。
> **一句话口诀**：下山要看路，步子别太大。

### 5.2.1 梯度下降：AI 是怎么“悔过”的

我们之前说，训练就是“犯错挨打”。
但关键是，挨打之后，怎么改？
这就要用到 **梯度下降（Gradient Descent）**。

想象一下：你被直升机扔到了一座崇山峻岭中，四周漆黑一片（高维空间），你的目标是找到海拔最低的山谷（Loss = 0）。
你看不见路，只能用脚试探周围的坡度（计算梯度）。
*   如果你感觉那边是下坡，就往那边迈一步。
*   这就是 **“更新参数”**。

只要你不停地往“下坡”的方向走，理论上总能走到谷底。

### 5.2.2 学习率：步子迈多大？

在下山过程中，有一个生死攸关的参数：**学习率（Learning Rate）**。
它决定了你一步迈多远。

*   **步子太大**（学习率高）：虽然走得快，但容易直接跨过山谷，跑到对面山上去了（震荡不收敛）。
*   **步子太小**（学习率低）：像蚂蚁挪窝一样，走了一万年还没走到谷底（训练太慢）。

所以，最好的策略是 **“先快后慢”**：在山顶时步伐大一点，快到底时步伐小一点，慢慢挪进去（学习率衰减）。

### 5.2.3 局部最优：陷在半山腰

梯度下降有一个致命的缺陷：**局部最优陷阱**。
有时候你以为你到底了（四周都是上坡），但其实你只是在一个半山腰的小坑里。真正的谷底还在几公里外。

为了解决这个问题，科学家发明了各种“魔改版”下山法，比如 **SGD（随机梯度下降）** 和 **Adam**。
它们就像给你的下山过程加了一点 **“惯性”（动量）**。
当你冲进小坑时，利用惯性直接冲出去，继续寻找真正的大海。

### 5.2.4 思考题

为什么现在的 AI 工程师，经常自嘲是 **“炼丹师”**？
因为调节这些超参数（学习率、层数、Batch Size）就像配火药一样，往往没有绝对的科学公式，全靠经验和直觉（还有运气）。
你觉得未来会有“AI 炼丹师”这个职业吗？还是说 AI 最终能学会“自己调节参数”（AutoML）？
